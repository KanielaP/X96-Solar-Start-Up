# Simple Environment

<img src="images/Basic Agent_Environment.png" align="middle" width="60%"/>

This is the basic environment that will be the baseline to train the agent. 

## Description

This environment randomly generates solar charge, temperature and load consumption that is sent to the agent to process. The initial battery level is randomly generated, but on reoccuring runs it will take the projected battery level value. The Capacity of the battery is taken from the global parameter which is set to 10kw/h.This process is expected every hour after the agent's action. 
<br> 

<!--
<br> sF_Expert was not able to fully understand. No set of data type to understand and have to hunt through imports
-->

## Action Space

The action is initializing the environment. The human interaction allows subsequent runs where the environment will update parameter values based on observation space.   

## Observation Space

-  Load(kw/h): House Consumption of Electricity
-  Solar(kw/h): Electricity Generated by Solar Panels
-  Capacity(kw/h): Max Capacty of Battery
-  Battery(kw/h): Current Battery Charge
-  Temperature(Â°C): Current Temperature of Weather
-  Reward(kw/h): Total Amount of kw/h from Current Session
-  Purchase Price ($/kw/h): Base Rates to Buy Electrcity
-  Selling Price ($/kw/h): Base Rates to Sell Electricity 

| Parameters | Values             | 
|------------|--------------------|
| Load       | RNG(0,8)           |
| Solar      | RNG(0,4)           | 
| Capacity   | 27                 | 
| Battery    | Battery Dynamix    | 
| Temperature| RNG(27,29)         | 
| Reward     | Reward             |

<br> RNG: Random Number Generator where first number is lowest possible value and second number is max possible value. 
<br> All random value observations are uniform.  

<!--
<br> Battery Projection = Previous Battery Value + Agent's Input Value.
<br> Reward Projection = Previous Reward Value + Agent's Input Value. 
--> 


## Starting State

| Parameters | Values         | 
|------------|----------------|
| Load       | 0              |
| Solar      | 0              | 
| Capacity   | 10             | 
| Battery    | RNG(0,10)      | 
| Temperature| RNG(27,29)     |   
| Reward     | 0              |  

## Battery Dynamix

Given an Action, the Battery follows the following transition dynamics:

*Battery Level<sub>t+1</sub>= Battery Level<sub>t</sub> + Action<sub>t+1</sub>*
<br>*If Battery level<sub>t+1</sub> > 27, then Battery level<sub>t+1</sub> = 27*
<br>*If Battery level<sub>t+1</sub> < 0, thhen Battery level<sub>t+1</sub> = 0*

Where the action is clipped to the range [-10,10] and the battery's level is clipped at the range [0,27]. t represents the state. 

## Reward


-  Action(kw/h): Charge Value given by Agent
-  Purchase Price ($/kw/h): Base Rates to Buy Electrcity
-  Selling Price ($/kw/h): Base Rates to Sell Electricity 

| Event                                   | Reward                                                |
|-----------------------------------------|-------------------------------------------------------|
|  -Load + Solar + Action < 0             | Reward = -Purchasing Price * (- Load + Solar+ Action) |
|  -Load + Solar + Action > 0             | Reward = Selling Rate * (- Load + Solar + Action)     | 

<!--
| Event                            | Values             | Reward                                                |
|----------------------------------|--------------------|-------------------------------------------------------|
| Battery + action < 0             | Battery = 0        | Reward = Previous Reward + Battery + Action           |
| Battery + action > Capacity      | Battery = Capacity | Reward = Previous Reward +Battery + Action - Capacity | 
-->


<br> Difference in the scenario will go to grid but will affect the reward.

## Environment End

Human input to rerun the code to generate new values to train the agent. There is no actual end other than quitting the program or hitting step run limit. 

## Arguments 


