# Simple Agent

<img src="images/Basic Agent_Environment.png" align="middle" width="60%"/>

This is the basic environment that will be the baseline to train the agent. 

## Description

This environment randomly generates solar charge, temperature and load consumption that is sent to the agent to process. The initial battery level is randomly generated, but on reoccuring runs it will take the projected battery level value. The Capacity of the battery is taken from the global parameter which is set to 10kw/h.This process is expected every hour after the agent's action. 

## Action Space

The action is initializing the environment. The human interaction allows subsequent runs where the environment will update parameter values based on observation space.   

## Observation Space

-  Load(kw/h): House Consumption of Electricity
-  Solar(kw/h): Electricity Generated by Solar Panels
-  Capacity(kw/h): Max Capacty of Battery
-  Battery(kw/h): Current Battery Charge
-  Temperature(Â°C): Current Temperature of Weather
-  Reward(kw/h): Total Amount of kw/h from Current Session

| Parameters | Values             | 
|------------|--------------------|
| Load       | RNG(0,8)           |
| Solar      | RNG(0,4)           | 
| Capacity   | 10                 | 
| Battery    | Battery Projection | 
| Temperature| RNG(27,29)         | 
| Reward     | Reward Projection  |

<br> RNG: Random Number Generator where first number is lowest possible value and second number is max possible value. 
<br> All random value observations are uniform.  
<br> Battery Projection = Previous Battery Value + Agent's Input Value.
<br> Reward Projection = Previous Reward Value + Agent's Input Value. 

## Starting State

| Parameters | Values         | 
|------------|----------------|
| Load       | 0              |
| Solar      | 0              | 
| Capacity   | 10             | 
| Battery    | RNG(0,10)      | 
| Temperature| RNG(27,29)     |   
| Reward     | 0              |  

## Override Policy

-  Action(kw/h): Charge Value given by Agent 

| Event                            | Values             | Reward                                                |
|----------------------------------|--------------------|-------------------------------------------------------|
| Battery + action < 0             | Battery = 0        | Reward = Previous Reward + Battery + Action           |
| Battery + action > Capacity      | Battery = Capacity | Reward = Previous Reward +Battery + Action - Capacity | 

<br> Difference in the scenario will go to grid but will affect the reward.

## Environment End

Human input to rerun the code to generate new values to train the agent. There is no actual end other than quitting the program. 

## Arguements 


